{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92dd7bdd",
   "metadata": {},
   "source": [
    "\n",
    "# MorphoTokenizer Test Harness (`test_harness.ipynb`)\n",
    "\n",
    "This notebook lets you **interactively tune** the parameters of `MorphoTokenizer`\n",
    "and see how they affect segmentation on a real biomedical PDF:\n",
    "\n",
    "> `nasal-congestion_ijgm-3-059.pdf`\n",
    "\n",
    "**Workflow**\n",
    "\n",
    "1. Adjust the parameters in **Cell 2** (`TOKENIZER_PARAMS`).\n",
    "2. Run the cells:\n",
    "   - The notebook will load the PDF from the same directory as the notebook.\n",
    "   - It will extract text and keep a sample chunk.\n",
    "3. Use the inspection cells to:\n",
    "   - Segment specific words (e.g., `hyperparathyroidism`, `thyroid`, `secondary`).\n",
    "   - See a side‑by‑side view of original vs. `<ETY>`‑marked text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b51ae03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: g:\\My Drive\\Education\\UMBC\\2025\\CMSC 691\\Project\n",
      "PDF path: g:\\My Drive\\Education\\UMBC\\2025\\CMSC 691\\Project\\nasal-congestion_ijgm-3-059.pdf exists? True\n",
      "Root lexicon: g:\\My Drive\\Education\\UMBC\\2025\\CMSC 691\\Project\\morph_lexicon_by_root.json.gz exists? True\n",
      "Proto lexicon: g:\\My Drive\\Education\\UMBC\\2025\\CMSC 691\\Project\\morph_lexicon_by_protoroot.json.gz exists? True\n"
     ]
    }
   ],
   "source": [
    "# !pip install -U pypdf --quiet\n",
    "\n",
    "# 1) Imports and file paths\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "from typing import List, Tuple\n",
    "\n",
    "# PDF reading\n",
    "try:\n",
    "    from pypdf import PdfReader\n",
    "except ImportError:\n",
    "    # If pypdf is not installed, uncomment this line in your environment:\n",
    "    # !pip install pypdf\n",
    "    from pypdf import PdfReader  # will still fail here if not installed\n",
    "\n",
    "# Our tokenizer module (must be in the same directory or on PYTHONPATH)\n",
    "from morpho_vtb_tokenizer import MorphoTokenizer\n",
    "\n",
    "# Base directory = directory of this notebook\n",
    "BASE_DIR = Path.cwd()\n",
    "\n",
    "PDF_NAME = \"nasal-congestion_ijgm-3-059.pdf\"\n",
    "PDF_PATH = BASE_DIR / PDF_NAME\n",
    "\n",
    "ROOT_LEX_NAME_GZ = \"morph_lexicon_by_root.json.gz\"\n",
    "PROTO_LEX_NAME_GZ = \"morph_lexicon_by_protoroot.json.gz\"\n",
    "\n",
    "ROOT_LEX_PATH = BASE_DIR / ROOT_LEX_NAME_GZ\n",
    "PROTO_LEX_PATH = BASE_DIR / PROTO_LEX_NAME_GZ\n",
    "\n",
    "print(\"Working directory:\", BASE_DIR)\n",
    "print(\"PDF path:\", PDF_PATH, \"exists?\", PDF_PATH.exists())\n",
    "print(\"Root lexicon:\", ROOT_LEX_PATH, \"exists?\", ROOT_LEX_PATH.exists())\n",
    "print(\"Proto lexicon:\", PROTO_LEX_PATH, \"exists?\", PROTO_LEX_PATH.exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d8a6ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current TOKENIZER_PARAMS:\n",
      "  lexicon_json: g:\\My Drive\\Education\\UMBC\\2025\\CMSC 691\\Project\\morph_lexicon_by_root.json.gz\n",
      "  proto_lexicon_json: g:\\My Drive\\Education\\UMBC\\2025\\CMSC 691\\Project\\morph_lexicon_by_protoroot.json.gz\n",
      "  max_morpheme_len: 12\n",
      "  unk_base_penalty: -2.0\n",
      "  unk_per_char: -0.2\n",
      "  add_generic_suffixes: True\n",
      "  lambda_penalty: 4.5\n",
      "  long_unsplit_min_len: 5\n",
      "  long_unsplit_penalty: 3.0\n",
      "  short2_adjust: -5.0\n",
      "  mid_root_adjust: 2.0\n",
      "  long_root_adjust: -0.5\n",
      "LONG_ROOT_THRESHOLD: 7\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2) Parameter block for MorphoTokenizer\n",
    "# You can edit these values and re-run the cell to tune behavior.\n",
    "#\n",
    "# Recommended range for the three adjustments: [-100, +100]\n",
    "\n",
    "TOKENIZER_PARAMS = dict(\n",
    "    lexicon_json=ROOT_LEX_PATH,\n",
    "    proto_lexicon_json=PROTO_LEX_PATH,\n",
    "    max_morpheme_len=12,          # hard cap on candidate substring length\n",
    "    unk_base_penalty=-2.0,        # base score for unknown pieces\n",
    "    unk_per_char=-0.2,            # per-character penalty for unknown pieces\n",
    "    add_generic_suffixes=True,\n",
    "    lambda_penalty=4.5,           # cost per piece (discourages over-segmentation)\n",
    "    long_unsplit_min_len=5,       # word length at which we start penalizing \"no split\"\n",
    "    long_unsplit_penalty=3.0,     # penalty if long word kept as a single piece\n",
    "\n",
    "    # Tunable adjustments (you can change these and re-run)\n",
    "    short2_adjust=-5.0,           # adjustment for 2-char roots (negative = penalize)\n",
    "    mid_root_adjust=+2.0,         # adjustment for mid-length roots (4–9 chars)\n",
    "    long_root_adjust=-0.5,        # per-char adjustment when length > LONG_ROOT_THRESHOLD\n",
    ")\n",
    "\n",
    "# Threshold length for \"long\" roots; this is used together with long_root_adjust.\n",
    "LONG_ROOT_THRESHOLD = 7\n",
    "\n",
    "print(\"Current TOKENIZER_PARAMS:\")\n",
    "for k, v in TOKENIZER_PARAMS.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "print(\"LONG_ROOT_THRESHOLD:\", LONG_ROOT_THRESHOLD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "210f6562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MorphoTokenizer is ready.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3) Instantiate MorphoTokenizer with the current parameters\n",
    "\n",
    "morpho_tok = MorphoTokenizer(\n",
    "    lexicon_json=TOKENIZER_PARAMS[\"lexicon_json\"],\n",
    "    proto_lexicon_json=TOKENIZER_PARAMS[\"proto_lexicon_json\"],\n",
    "    max_morpheme_len=TOKENIZER_PARAMS[\"max_morpheme_len\"],\n",
    "    unk_base_penalty=TOKENIZER_PARAMS[\"unk_base_penalty\"],\n",
    "    unk_per_char=TOKENIZER_PARAMS[\"unk_per_char\"],\n",
    "    add_generic_suffixes=TOKENIZER_PARAMS[\"add_generic_suffixes\"],\n",
    "    lambda_penalty=TOKENIZER_PARAMS[\"lambda_penalty\"],\n",
    "    long_unsplit_min_len=TOKENIZER_PARAMS[\"long_unsplit_min_len\"],\n",
    "    long_unsplit_penalty=TOKENIZER_PARAMS[\"long_unsplit_penalty\"],\n",
    "    short2_adjust=TOKENIZER_PARAMS[\"short2_adjust\"],\n",
    "    mid_root_adjust=TOKENIZER_PARAMS[\"mid_root_adjust\"],\n",
    "    long_root_adjust=TOKENIZER_PARAMS[\"long_root_adjust\"],\n",
    ")\n",
    "\n",
    "print(\"MorphoTokenizer is ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "439a27fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4) Helper: morpho_preseg(text) and visualization utilities\n",
    "\n",
    "def _tokenize_with_separators(text: str):\n",
    "    # Split text into tokens, keeping whitespace and punctuation as separate tokens.\n",
    "    return re.findall(r\"\\w+|\\s+|[^\\w\\s]\", text, flags=re.UNICODE)\n",
    "\n",
    "\n",
    "def morpho_preseg(text: str) -> str:\n",
    "    # Insert <ETY> markers on morphological roots for all alphabetic tokens.\n",
    "    tokens = _tokenize_with_separators(text)\n",
    "    out_parts = []\n",
    "    for t in tokens:\n",
    "        if t.isalpha():\n",
    "            pieces = morpho_tok.segment_word(t)\n",
    "            chunk = \"\"\n",
    "            for p in pieces:\n",
    "                if p.kind == \"root\":\n",
    "                    chunk += f\"{p.text}<ETY>\"\n",
    "                else:\n",
    "                    chunk += p.text\n",
    "            out_parts.append(chunk or t)\n",
    "        else:\n",
    "            out_parts.append(t)\n",
    "    return \"\".join(out_parts)\n",
    "\n",
    "\n",
    "def show_word_segmentation(words):\n",
    "    # Pretty-print segmentation of one or more words.\n",
    "    if isinstance(words, str):\n",
    "        words = [words]\n",
    "    for w in words:\n",
    "        pieces = morpho_tok.segment_word(w)\n",
    "        label = \" + \".join(f\"{p.text}[{p.kind}:{p.score:.2f}]\" for p in pieces)\n",
    "        print(f\"{w} -> {label}\")\n",
    "\n",
    "\n",
    "def show_side_by_side(text: str, max_chars: int = 400):\n",
    "    # Show original vs. morpho_preseg(text) for a short snippet.\n",
    "    snippet = text[:max_chars]\n",
    "    seg = morpho_preseg(snippet)\n",
    "    print(\"ORIGINAL:\")\n",
    "    print(snippet)\n",
    "    print(\"\\nSEGMENTED (with <ETY>):\")\n",
    "    print(seg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a55eec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text length: 26515\n",
      "First 500 characters:\n",
      "\n",
      "International Journal of General Medicine 2010:3 61\n",
      "Nasal congestion diagnosisDovepress\n",
      "submit your manuscript | www.dovepress.com\n",
      "Dovepress \n",
      "T able 1 Differential diagnosis of nasal congestion\n",
      "Rhinitis Duration T ypical other symptoms\n",
      "Infectious rhinitis\n",
      " viral 10 days Sneezing, watery rhinorrhea, sore throat  \n",
      "Purulent discharge, facial pain\n",
      " Bacterial 10 days\n",
      " Other infectious agents 10 days\n",
      "Allergic rhinitis\n",
      " Intermittent 4 days/week For all allergic rhinitis: Sneezing, watery rhinorrhea\n"
     ]
    }
   ],
   "source": [
    "# 5) Load text from the PDF and keep a sample chunk\n",
    "\n",
    "if not PDF_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Expected PDF at {PDF_PATH}, but it does not exist.\")\n",
    "\n",
    "reader = PdfReader(str(PDF_PATH))\n",
    "\n",
    "all_text_pages = []\n",
    "\n",
    "# --- Skip the first 2 pages ---\n",
    "# Start from reader.pages[2:] instead of reader.pages\n",
    "for page_idx, page in enumerate(reader.pages[2:], start=1):\n",
    "    try:\n",
    "        page_text = page.extract_text() or \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: could not extract text from page {page_idx}: {e}\")\n",
    "        page_text = \"\"\n",
    "    all_text_pages.append(page_text)\n",
    "\n",
    "full_text = \"\\n\\n\".join(all_text_pages)\n",
    "\n",
    "print(\"Total text length:\", len(full_text))\n",
    "print(\"First 500 characters:\\n\")\n",
    "print(full_text[:500])\n",
    "\n",
    "# For convenience, keep a shorter sample for repeated experiments.\n",
    "SAMPLE_TEXT = full_text[:5000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0428a212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Word-level segmentation ===\n",
      "hyperparathyroidism -> hyper[root:7.60] + para[root:7.30] + thyroid[root:7.79] + ism[root:5.00]\n",
      "thyroid -> thyroid[root:7.79]\n",
      "secondary -> second[root:7.49] + ary[root:4.59]\n",
      "oxacalcitriol -> oxa[unk:-2.60] + cal[root:5.00] + citri[root:7.19] + ol[root:-0.30]\n",
      "biomedical -> bio[root:5.29] + medical[root:7.79]\n",
      "\n",
      "=== morpho_preseg('hyperparathyroidism') ===\n",
      "hyper<ETY>para<ETY>thyroid<ETY>ism<ETY>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 6) Quick sanity checks on key words\n",
    "\n",
    "words_to_check = [\n",
    "    \"hyperparathyroidism\",\n",
    "    \"thyroid\",\n",
    "    \"secondary\",\n",
    "    \"oxacalcitriol\",\n",
    "    \"biomedical\",\n",
    "]\n",
    "\n",
    "print(\"=== Word-level segmentation ===\")\n",
    "show_word_segmentation(words_to_check)\n",
    "\n",
    "print(\"\\n=== morpho_preseg('hyperparathyroidism') ===\")\n",
    "print(morpho_preseg(\"hyperparathyroidism\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b42d41c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL:\n",
      "International Journal of General Medicine 2010:3 61\n",
      "Nasal congestion diagnosisDovepress\n",
      "submit your manuscript | www.dovepress.com\n",
      "Dovepress \n",
      "T able 1 Differential diagnosis of nasal congestion\n",
      "Rhinitis Duration T ypical other symptoms\n",
      "Infectious rhinitis\n",
      " viral 10 days Sneezing, watery rhinorrhea, sore throat  \n",
      "Purulent discharge, facial pain\n",
      " Bacterial 10 days\n",
      " Other infectious agents 10 days\n",
      "Allergic rhinitis\n",
      " Intermittent 4 days/week For all allergic rhinitis: Sneezing, watery rhinorrhea,  \n",
      "itch, eye symptoms, lower airway symptoms\n",
      "4 weeks/year\n",
      " Persistent 4 days/week\n",
      "4 weeks/year\n",
      "Occupational rhinitis\n",
      " Allergic\n",
      "Usually less when away from workplace As allergic rhinitis\n",
      " Nonallergic Usually mainly nasal blockage, rhinorrhea\n",
      "Drug induced Sometimes difficult to make relation,  \n",
      "ca\n",
      "\n",
      "SEGMENTED (with <ETY>):\n",
      "Inter<ETY>national<ETY> Journal<ETY> of<ETY> General<ETY> Medicine<ETY> 2010:3 61\n",
      "Nasal<ETY> con<ETY>gestion<ETY> dia<ETY>gnosis<ETY>Dove<ETY>press<ETY>\n",
      "sub<ETY>mit<ETY> your<ETY> manu<ETY>script<ETY> | www<ETY>.dove<ETY>press<ETY>.com<ETY>\n",
      "Dove<ETY>press<ETY> \n",
      "T able<ETY> 1 Differ<ETY>ent<ETY>ial dia<ETY>gnosis<ETY> of<ETY> nasal<ETY> con<ETY>gestion<ETY>\n",
      "Rhinitis<ETY> Dura<ETY>tion<ETY> T ypical<ETY> other<ETY> symptom<ETY>s\n",
      "Infect<ETY>iou<ETY>s rhinitis<ETY>\n",
      " viral<ETY> 10 days<ETY> Snee<ETY>zing<ETY>, watery<ETY> rhino<ETY>rrhea<ETY>, sore<ETY> throat<ETY>  \n",
      "Purulent<ETY> dis<ETY>charge<ETY>, facial<ETY> pain<ETY>\n",
      " Bacterial<ETY> 10 days<ETY>\n",
      " Other<ETY> infect<ETY>iou<ETY>s agent<ETY>s 10 days<ETY>\n",
      "All<ETY>ergic<ETY> rhinitis<ETY>\n",
      " Inter<ETY>mit<ETY>tent<ETY> 4 days<ETY>/week<ETY> For<ETY> all<ETY> all<ETY>ergic<ETY> rhinitis<ETY>: Snee<ETY>zing<ETY>, watery<ETY> rhino<ETY>rrhea<ETY>,  \n",
      "itch<ETY>, eye<ETY> symptom<ETY>s, lower<ETY> air<ETY>way<ETY> symptom<ETY>s\n",
      "4 week<ETY>s/year<ETY>\n",
      " Pers<ETY>ist<ETY>ent<ETY> 4 days<ETY>/week<ETY>\n",
      "4 week<ETY>s/year<ETY>\n",
      "Occupational<ETY> rhinitis<ETY>\n",
      " All<ETY>ergic<ETY>\n",
      "Usually<ETY> less<ETY> when<ETY> away<ETY> from<ETY> work<ETY>place<ETY> As<ETY> all<ETY>ergic<ETY> rhinitis<ETY>\n",
      " Non<ETY>all<ETY>ergic<ETY> Usually<ETY> mainly<ETY> nasal<ETY> bloc<ETY>kage<ETY>, rhino<ETY>rrhea<ETY>\n",
      "Drug<ETY> induce<ETY>d Some<ETY>times<ETY> difficult<ETY> to<ETY> make<ETY> relation<ETY>,  \n",
      "ca<ETY>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 7) Side-by-side view on a snippet from the PDF\n",
    "\n",
    "show_side_by_side(SAMPLE_TEXT, max_chars=800)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
